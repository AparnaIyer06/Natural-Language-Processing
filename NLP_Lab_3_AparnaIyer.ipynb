{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Name: Aparna Iyer\n",
        "\n",
        "PRN: 22070126017\n",
        "\n",
        "Division: AI-ML A1\n",
        "\n",
        "Batch: 2022-2026"
      ],
      "metadata": {
        "id": "dkNKMC1_19ef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementation of Count Vectorizer and TF-IDF Vectorizer from scratch**"
      ],
      "metadata": {
        "id": "yCitk-EaZu47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. Count Vectorizer"
      ],
      "metadata": {
        "id": "NA7F5bMnCR3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# Step 1: Tokenization\n",
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "# Step 2: Build Vocabulary\n",
        "def build_vocabulary(corpus):\n",
        "    vocabulary = set()\n",
        "    for document in corpus:\n",
        "        tokens = tokenize(document)\n",
        "        vocabulary.update(tokens)\n",
        "    return sorted(vocabulary)\n",
        "\n",
        "# Step 3: Create Document-Term Matrix\n",
        "def count_vectorizer(corpus):\n",
        "    vocabulary = build_vocabulary(corpus)\n",
        "    vocab_index = {word: i for i, word in enumerate(vocabulary)}\n",
        "    document_term_matrix = np.zeros((len(corpus), len(vocabulary)), dtype=int)\n",
        "\n",
        "    for doc_index, document in enumerate(corpus):\n",
        "        tokens = tokenize(document)\n",
        "        token_counts = defaultdict(int)\n",
        "        for token in tokens:\n",
        "            token_counts[token] += 1\n",
        "\n",
        "        for token, count in token_counts.items():\n",
        "            if token in vocab_index:\n",
        "                index = vocab_index[token]\n",
        "                document_term_matrix[doc_index][index] = count\n",
        "\n",
        "    return document_term_matrix, vocabulary\n",
        "\n",
        "# Example usage\n",
        "corpus = [\"This is a sample document\", \"This document is another example document\"]\n",
        "dtm, vocab = count_vectorizer(corpus)\n",
        "\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"Document-Term Matrix:\\n\", dtm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEflb80tZ1do",
        "outputId": "706bbbf8-2702-4a50-fb6d-ae680f0c0d45"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['a', 'another', 'document', 'example', 'is', 'sample', 'this']\n",
            "Document-Term Matrix:\n",
            " [[1 0 1 0 1 1 1]\n",
            " [0 1 2 1 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. TF-IDF Vectorizer:"
      ],
      "metadata": {
        "id": "GrvyMuSjCU7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# Step 1: Tokenization\n",
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "# Step 2: Build Vocabulary\n",
        "def build_vocabulary(corpus):\n",
        "    vocabulary = set()\n",
        "    for document in corpus:\n",
        "        tokens = tokenize(document)\n",
        "        vocabulary.update(tokens)\n",
        "    return sorted(vocabulary)\n",
        "\n",
        "# Step 3: Compute Term Frequency (TF)\n",
        "def compute_tf(document, vocab_index):\n",
        "    tokens = tokenize(document)\n",
        "    token_counts = defaultdict(int)\n",
        "    for token in tokens:\n",
        "        token_counts[token] += 1\n",
        "    total_tokens = len(tokens)\n",
        "\n",
        "    tf = np.zeros(len(vocab_index))\n",
        "    for token, count in token_counts.items():\n",
        "        if token in vocab_index:\n",
        "            index = vocab_index[token]\n",
        "            tf[index] = count / total_tokens\n",
        "    return tf\n",
        "\n",
        "# Step 4: Compute Inverse Document Frequency (IDF)\n",
        "def compute_idf(corpus, vocab_index):\n",
        "    num_docs = len(corpus)\n",
        "    idf = np.zeros(len(vocab_index))\n",
        "    for token in vocab_index:\n",
        "        count = sum(1 for document in corpus if token in tokenize(document))\n",
        "        idf[vocab_index[token]] = np.log((num_docs + 1) / (count + 1)) + 1\n",
        "    return idf\n",
        "\n",
        "# Step 5: Compute TF-IDF\n",
        "def tfidf_vectorizer(corpus):\n",
        "    vocabulary = build_vocabulary(corpus)\n",
        "    vocab_index = {word: i for i, word in enumerate(vocabulary)}\n",
        "    document_term_matrix = np.zeros((len(corpus), len(vocabulary)))\n",
        "\n",
        "    idf = compute_idf(corpus, vocab_index)\n",
        "\n",
        "    for doc_index, document in enumerate(corpus):\n",
        "        tf = compute_tf(document, vocab_index)\n",
        "        document_term_matrix[doc_index] = tf * idf\n",
        "\n",
        "    return document_term_matrix, vocabulary\n",
        "\n",
        "# Example usage\n",
        "corpus = [\"This is a sample document\", \"This document is another example document\"]\n",
        "tfidf_matrix, vocab = tfidf_vectorizer(corpus)\n",
        "\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"TF-IDF Matrix:\\n\", tfidf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJJiAWcVCFD9",
        "outputId": "09b4f4ea-788c-4340-b5db-62e7b5fdb59e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['a', 'another', 'document', 'example', 'is', 'sample', 'this']\n",
            "TF-IDF Matrix:\n",
            " [[0.28109302 0.         0.2        0.         0.2        0.28109302\n",
            "  0.2       ]\n",
            " [0.         0.23424418 0.33333333 0.23424418 0.16666667 0.\n",
            "  0.16666667]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Count Vectorizer and TF-IDF Vectorizer from the sklearn library**"
      ],
      "metadata": {
        "id": "8h-DrQ_fZ-5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. CountVectorizer from sklearn"
      ],
      "metadata": {
        "id": "0jm-ep_NC1dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Example corpus\n",
        "corpus = [\"This is a sample document\", \"This document is another example document\"]\n",
        "\n",
        "# Initialize the CountVectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the corpus\n",
        "X = count_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Get the vocabulary (feature names)\n",
        "vocabulary = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert to array for better readability\n",
        "document_term_matrix = X.toarray()\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"Document-Term Matrix:\\n\", document_term_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z2F0MZuZ9PM",
        "outputId": "c28b779a-e976-4788-9568-5554b9585229"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['another' 'document' 'example' 'is' 'sample' 'this']\n",
            "Document-Term Matrix:\n",
            " [[0 1 0 1 1 1]\n",
            " [1 2 1 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. TfidfVectorizer from Sklearn"
      ],
      "metadata": {
        "id": "K5Y0bOb-C5QX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Example corpus\n",
        "corpus = [\"This is a sample document\", \"This document is another example document\"]\n",
        "\n",
        "# Initialize the TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the corpus\n",
        "X = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Get the vocabulary (feature names)\n",
        "vocabulary = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert to array for better readability\n",
        "tfidf_matrix = X.toarray()\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"TF-IDF Matrix:\\n\", tfidf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUTB22J6CsnW",
        "outputId": "5f9c9bc3-91e7-41ea-a5e1-3894f5be5de2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['another' 'document' 'example' 'is' 'sample' 'this']\n",
            "TF-IDF Matrix:\n",
            " [[0.         0.44832087 0.         0.44832087 0.63009934 0.44832087]\n",
            " [0.44554752 0.63402146 0.44554752 0.31701073 0.         0.31701073]]\n"
          ]
        }
      ]
    }
  ]
}